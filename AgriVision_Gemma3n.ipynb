{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04ede09",
   "metadata": {},
   "source": [
    "# üå± AgriVision-Gemma: Vision‚ÄëLLM Fine‚ÄëTuning Notebook  \n",
    "**Fine‚Äëtuning the Gemma‚Äë3n‚Äëe2b‚Äëit model for crop disease diagnosis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02808df1",
   "metadata": {},
   "source": [
    "## üîñ Table of Contents\n",
    "1. üß∞ [Setup and Installation](#1-setup-and-installation)  \n",
    "2. üì¶ [Importing Libraries](#2-importing-libraries)  \n",
    "3. ‚öôÔ∏è [Configuration](#3-configuration)  \n",
    "4. üóÇÔ∏è [Data Preparation](#4-data-preparation)  \n",
    "5. ü§ñ [Model + QLoRA Configuration](#5-model-loading-and-qlora-configuration)  \n",
    "6. üöÄ [Training](#6-training)  \n",
    "7. üíæ [Saving Model](#7-saving-model)  \n",
    "8. üß™ [Inference](#8-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YJZZfARVyamz",
   "metadata": {
    "id": "YJZZfARVyamz"
   },
   "source": [
    "<a id=\"1-setup-and-installation\"></a>\n",
    "# 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c05224-7de4-433a-a936-b2902616e895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:44:27.492651Z",
     "iopub.status.busy": "2025-07-12T02:44:27.492104Z",
     "iopub.status.idle": "2025-07-12T02:50:11.403056Z",
     "shell.execute_reply": "2025-07-12T02:50:11.402108Z",
     "shell.execute_reply.started": "2025-07-12T02:44:27.492626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade unsloth\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install transformers==4.53.0\n",
    "!pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n_6A2V1myam3",
   "metadata": {
    "id": "n_6A2V1myam3"
   },
   "source": [
    "<a id=\"2-importing-libraries\"></a>\n",
    "\n",
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vHHHqn5ryam3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:11.404945Z",
     "iopub.status.busy": "2025-07-12T02:50:11.404586Z",
     "iopub.status.idle": "2025-07-12T02:50:44.307959Z",
     "shell.execute_reply": "2025-07-12T02:50:44.307125Z",
     "shell.execute_reply.started": "2025-07-12T02:50:11.404908Z"
    },
    "id": "vHHHqn5ryam3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 02:50:19.776341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752288619.962496      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752288620.014335      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastModel\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset \n",
    "from transformers import TrainingArguments \n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText \n",
    "from PIL import Image\n",
    "import re\n",
    "import requests\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFtE3Y0ryam4",
   "metadata": {
    "id": "XFtE3Y0ryam4"
   },
   "source": [
    "<a id=\"3-configuration\"></a>\n",
    "\n",
    "# 3. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m1I5xa-F72a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:44.309026Z",
     "iopub.status.busy": "2025-07-12T02:50:44.308789Z",
     "iopub.status.idle": "2025-07-12T02:50:44.437379Z",
     "shell.execute_reply": "2025-07-12T02:50:44.436814Z",
     "shell.execute_reply.started": "2025-07-12T02:50:44.309009Z"
    },
    "id": "m1I5xa-F72a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"unsloth/gemma-3n-e2b-it-unsloth-bnb-4bit\"\n",
    "output_dir = \"/kaggle/working/gemma-qlora-finetuned-cddm\"\n",
    "\n",
    "hub_model_id = \"shreyansh24/gemma3n-e2b-cddm-finetune\"\n",
    "hub_private_repo = False\n",
    "\n",
    "hf_token=\"YOUR_HF_TOKEN\"\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GMYsegVZyam5",
   "metadata": {
    "id": "GMYsegVZyam5"
   },
   "source": [
    "<a id=\"4-data-preparation\"></a>\n",
    "\n",
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a1f3fa-152a-4135-96a6-393d96fbf43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:44.439508Z",
     "iopub.status.busy": "2025-07-12T02:50:44.439290Z",
     "iopub.status.idle": "2025-07-12T02:50:44.443091Z",
     "shell.execute_reply": "2025-07-12T02:50:44.442526Z",
     "shell.execute_reply.started": "2025-07-12T02:50:44.439487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT_DIR = \"/kaggle/input/crop-disease-data/\"\n",
    "dataset_json_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset/Crop_Disease_train_qwenvl.json\")\n",
    "system_message = \"You are a helpful AI assistant specialized in crop disease diagnosis. Provide concise and accurate information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccwLJVcH7XbG",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:44.444346Z",
     "iopub.status.busy": "2025-07-12T02:50:44.444120Z",
     "iopub.status.idle": "2025-07-12T02:50:44.460409Z",
     "shell.execute_reply": "2025-07-12T02:50:44.459824Z",
     "shell.execute_reply.started": "2025-07-12T02:50:44.444320Z"
    },
    "id": "ccwLJVcH7XbG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper function to extract PIL Images from the TRL-formatted messages\n",
    "# This is used by the `collate_fn`.\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and element.get(\"type\") == \"image\":\n",
    "                if \"image\" in element and isinstance(element[\"image\"], Image.Image): # Check if it's actually a PIL Image object\n",
    "                    image_inputs.append(element[\"image\"].convert(\"RGB\"))\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "U1aj4jSc7XYn",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:44.461444Z",
     "iopub.status.busy": "2025-07-12T02:50:44.461163Z",
     "iopub.status.idle": "2025-07-12T02:50:44.477424Z",
     "shell.execute_reply": "2025-07-12T02:50:44.476746Z",
     "shell.execute_reply.started": "2025-07-12T02:50:44.461427Z"
    },
    "id": "U1aj4jSc7XYn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to transform CDDM format to TRL multimodal chat format (Gemma style)\n",
    "def format_data_for_trl(sample):\n",
    "    trl_messages = []\n",
    "\n",
    "    # 1. Add system message at the beginning of each conversation (Keep as is)\n",
    "    trl_messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "    })\n",
    "\n",
    "    # Process each turn in the conversation\n",
    "    for i, turn in enumerate(sample['conversations']):\n",
    "        # Map Qwen-VL roles to Gemma roles (Keep as is)\n",
    "        current_role = \"user\" if turn[\"from\"] == \"user\" else \"assistant\"\n",
    "        content_value = turn[\"value\"]\n",
    "\n",
    "        content_parts = []\n",
    "\n",
    "        img_pattern = r'<img>(.*?)</img>'\n",
    "        img_match = re.search(img_pattern, content_value)\n",
    "\n",
    "        # Handle the case where an image is present in the user's turn\n",
    "        if img_match and current_role == \"user\": # An image should only appear in a user turn\n",
    "            image_path_in_text = img_match.group(1) # Path inside <img> tag (e.g., \"/dataset/images/Rice,Blast/plant_121175.jpg\")\n",
    "\n",
    "            # --- CRITICAL CORRECTION: Construct the ABSOLUTE image path from Kaggle Input ---\n",
    "            # Remove leading slash from image_path_in_text if present\n",
    "            # This ensures that os.path.join works correctly.\n",
    "            cleaned_image_path_in_text = image_path_in_text.lstrip('/')\n",
    "            # Join the Kaggle input root with the cleaned relative path\n",
    "            corrected_image_path = os.path.join(KAGGLE_INPUT_DIR, cleaned_image_path_in_text)\n",
    "\n",
    "\n",
    "            loaded_image = None\n",
    "            # Use the correctly constructed path to check if the file exists\n",
    "            if os.path.exists(corrected_image_path):\n",
    "                try:\n",
    "                    # Try opening the image file\n",
    "                    loaded_image = Image.open(corrected_image_path).convert(\"RGB\")\n",
    "                    content_parts.append({\"type\": \"image\", \"image\": loaded_image}) # Store the PIL Image object\n",
    "                except Exception as e:\n",
    "                    # This warning indicates the file exists but couldn't be opened as an image\n",
    "                    print(f\"Warning: Could not load image {corrected_image_path}: {e}. Adding path as text for sample ID: {sample.get('id', 'N/A')}\")\n",
    "                    content_parts.append({\"type\": \"text\", \"text\": f\"Error loading image: {corrected_image_path}\"})\n",
    "            else:\n",
    "                # This warning indicates the file was not found at the expected path\n",
    "                print(f\"Warning: Image file not found at {corrected_image_path}. Adding path as text for sample ID: {sample.get('id', 'N/A')}\")\n",
    "                content_parts.append({\"type\": \"text\", \"text\": f\"Image not found: {corrected_image_path}\"})\n",
    "\n",
    "            # Extract text after removing the <img> tag and \"Picture X:\" prefix (Keep as is)\n",
    "            text_part = re.sub(img_pattern, \"\", content_value)\n",
    "            text_part = re.sub(r'^(Picture\\s+\\d+:\\s*)', '', text_part).strip()\n",
    "\n",
    "            if text_part:\n",
    "                content_parts.append({\"type\": \"text\", \"text\": text_part})\n",
    "            # If no text and image failed, ensure content_parts is not empty (Keep as is)\n",
    "            elif not content_parts and loaded_image is None:\n",
    "                 content_parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "\n",
    "        else:\n",
    "            # All other turns are text-only (Keep as is)\n",
    "            cleaned_text = turn[\"value\"].strip()\n",
    "            if cleaned_text:\n",
    "                content_parts.append({\"type\": \"text\", \"text\": cleaned_text})\n",
    "            elif current_role == \"assistant\":\n",
    "                content_parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "\n",
    "        if content_parts:\n",
    "            trl_messages.append({\"role\": current_role, \"content\": content_parts})\n",
    "\n",
    "    # Ensure the messages list is not empty (at least system message should be there) (Keep as is)\n",
    "    if not trl_messages:\n",
    "        return {\"messages\": [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}]}\n",
    "\n",
    "    return {\"messages\": trl_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "_sCaX97a4Bbz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:44.478494Z",
     "iopub.status.busy": "2025-07-12T02:50:44.478254Z",
     "iopub.status.idle": "2025-07-12T02:50:49.928375Z",
     "shell.execute_reply": "2025-07-12T02:50:49.927794Z",
     "shell.execute_reply.started": "2025-07-12T02:50:44.478473Z"
    },
    "id": "_sCaX97a4Bbz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the JSON file\n",
    "with open(dataset_json_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c941-dba9-4655-8b15-1d54fcb53475",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T02:50:49.929464Z",
     "iopub.status.busy": "2025-07-12T02:50:49.929229Z",
     "iopub.status.idle": "2025-07-12T02:52:15.475285Z",
     "shell.execute_reply": "2025-07-12T02:52:15.474325Z",
     "shell.execute_reply.started": "2025-07-12T02:50:49.929444Z"
    },
    "id": "3H65VsgZ7XWI",
    "outputId": "89a1b024-b18d-4ea8-d080-8d56a4965e0a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a helpful AI assistant specialized in crop disease diagnosis. Provide concise and accurate information.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'image',\n",
       "     'image': <PIL.Image.Image image mode=RGB size=256x256>},\n",
       "    {'type': 'text', 'text': 'What are the features of this picture?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'This image shows a tomato leaf that exhibits symptoms of Yellow Leaf Curl Virus, characterized by yellowing and curling of the leaves.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text', 'text': \"What plant's leaf is this?\"}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text', 'text': 'This is a tomato leaf.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text', 'text': 'Is this crop diseased?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Yes, this tomato leaf is afflicted with Yellow Leaf Curl Virus.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text', 'text': 'Is this apple leaf healthy?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"This is not an apple leaf; it's a tomato leaf afflicted with Yellow Leaf Curl Virus.\"}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Is this leaf from a cucumber plant?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text', 'text': 'No, this is a tomato leaf.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Has this leaf contracted Early Blight?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'No, this tomato leaf has Yellow Leaf Curl Virus.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Is this a tomato leaf or a potato leaf?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text', 'text': 'This is a tomato leaf.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'What disease does this tomato leaf have?'}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'This tomato leaf is affected by Yellow Leaf Curl Virus.'}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_subset=66000 \n",
    "end_subset = 76000\n",
    "\n",
    "train_dataset = [format_data_for_trl(sample) for sample in data[start_subset:end_subset]]\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "KZzvplNFmMBx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T02:52:15.476491Z",
     "iopub.status.busy": "2025-07-12T02:52:15.476191Z",
     "iopub.status.idle": "2025-07-12T02:52:15.484578Z",
     "shell.execute_reply": "2025-07-12T02:52:15.483696Z",
     "shell.execute_reply.started": "2025-07-12T02:52:15.476450Z"
    },
    "id": "KZzvplNFmMBx",
    "outputId": "7b7a25aa-9b3f-4f30-a12a-9737d4bca0ba",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of image loading for a user turn:\n",
      "  Image element found. PIL Image object: <PIL.Image.Image image mode=RGB size=256x256 at 0x7EA0B6C9DB10>\n"
     ]
    }
   ],
   "source": [
    "# Find a sample with an image in a user turn and print its content\n",
    "print(f\"\\nExample of image loading for a user turn:\")\n",
    "image_sample_found = False\n",
    "for sample in train_dataset:\n",
    "    for msg in sample['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            for content_elem in msg['content']:\n",
    "                if content_elem['type'] == 'image':\n",
    "                    print(f\"  Image element found. PIL Image object: {content_elem['image']}\")\n",
    "                    image_sample_found = True\n",
    "                    break\n",
    "        if image_sample_found:\n",
    "            break\n",
    "    if image_sample_found:\n",
    "        break\n",
    "if not image_sample_found:\n",
    "    print(\"No image found in a user turn in the first few samples to demonstrate PIL object loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G0mG6ISxyanA",
   "metadata": {
    "id": "G0mG6ISxyanA"
   },
   "source": [
    "<a id=\"5-model-loading-and-qlora-configuration\"></a>\n",
    "\n",
    "# 5. Model Loading and QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ambmHu8oCn_w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "4fbae92a319745e39ac1469250911166",
      "c3838587e1aa444f96a45e02aa8efb16",
      "08e0fbea3ec048b08692fc49fb5f8f1b",
      "4c6c45eba64948ffb7d86ea91c5a1a36",
      "7387be7344124ff69378b7de40b82df1",
      "1bfb129a7b53459fbeca9cb852bfdc0e",
      "46e7ffdbda154942b519cfc2128abbec",
      "2f941417b0104810810229eb8c162b3d",
      "d015253d6fbc407a9f262cf0f69db370",
      "fc8304ba251041b59c397147e7d411c3",
      "93cdf0e645454124bfe33caea6c0a1f3",
      "278ce21991ef450596aa57bfd5ab16d2",
      "5890241611be4bddbb81569dd3ed5359",
      "cbbbde4cede045789790c60d1da79675",
      "8b08bda6c214479cb86a6f739ac25729",
      "b6ae5063a234489e9168aa10a22a1a1c",
      "d9fbe49579d54bf28dbb872fc86e1222",
      "34d23eb2c3b54809908f7646f08e74df",
      "c800e3e5683a4d828a213a0b061f4d34",
      "606e769bfb514c1ea1e58eb5de46f845",
      "3219089f16db44ecab6c65ef1ad69354",
      "8e8306b1c85a418d917167374ed56290",
      "8a7482187e4847939817cce500dcaec8",
      "a193a63bc41f415189bec95f102fb2f8",
      "49a1e2e5d48f41f4ad62098227728861",
      "5f1a6efff66741cb8454160d1d681858",
      "7d45161af70f4ef0a72b694d138ee46c",
      "b1c6fd187bee47e186c86024e2b5ae85",
      "0ddeda906c1f4636814ca781a29e8934",
      "927c7f67f4af416190f6942bcbc7f17b",
      "58634c145c29410d8eaf505bac7d69fc",
      "1566248ed4034b5eb9a04bcee66dfc10",
      "8f17dd001efa4bd7b0165fa83753ecc3",
      "510cc709181a48daabe47b13d8fff5fa",
      "27c3a9173efd401ba9b339e1abf53039",
      "08a036c9f70247af8105d765044831ac",
      "fa9c377236f540bc8ce363a19a55c9f2",
      "3b4c742b9c6f472b91ac52715cbee75b",
      "611be992b5b14bc58e6bd5af661158f6",
      "ae492047974742f4a03fe8bc751bf28c",
      "d5d655690e994f94b4a1024dd62ef07a",
      "f4ade5b31cfc4b9f95b9881fc098a8d5",
      "eb5ec0012ba94a6c9542fe42ff0786c9",
      "a101f8cc747e41539f0376a5d7c51f6b",
      "69c89c8412984805aed3b1f0390875ec",
      "5d6d0f69f60f416da2d679ea80649bd8",
      "3b07c6bb5c3349abab4babc057ae4881",
      "6abba4ef35194b4b8aa1b3f532013a93",
      "83c05b39e818435e879df255572a2aea",
      "7534434c6a474b8e8e205daa6b49a9ed",
      "e17537eae648486f8c1a92f9a2da7cd6",
      "e665502b535c4f0b94a2792ef1c0a9bc",
      "5a755318d3464dc6aabc7e40dd98da95",
      "3e0c364599ca4bb6be064c9b9a24a70a",
      "d171723a973b472cbb4c99641ea9103a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T02:52:15.487117Z",
     "iopub.status.busy": "2025-07-12T02:52:15.486907Z",
     "iopub.status.idle": "2025-07-12T02:53:21.177157Z",
     "shell.execute_reply": "2025-07-12T02:53:21.176542Z",
     "shell.execute_reply.started": "2025-07-12T02:52:15.487102Z"
    },
    "id": "ambmHu8oCn_w",
    "outputId": "1e34f6ad-c849-45e9-e7c4-e19420c80c29",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.7.3: Fast Gemma3N patching. Transformers: 4.53.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7c66606a85448a8dd7bc13018b7883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a1e031a46740749af3ead5f6e2a1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/2.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3938ce8b72d84e6db219ea64744bf9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea882988370e468388302876966d62c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/469M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66af363d6fee415e9e35fb708bef6ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1980aab1f9d24951aabba68dbb11676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b3c14958a84ed3936b3279dcb2726f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5ac02440f243efac0e9e7333292908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1a454565dd482d934f4f9632d2018f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd93d28ea6943d1812f4fa5b209e4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d276bbca84b447508c5492ee53485499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e09a6559bba425e985cecfa3d999f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4babfa6d7794c52950eb87346b45de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Unsloth Model Loading ---\n",
    "# Unsloth simplifies model loading and quantization.\n",
    "# It automatically sets up 4-bit quantization (QLoRA) and prepares the model for training.\n",
    "model, processor = FastModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.float16, # Use bfloat16 for computation\n",
    "    load_in_4bit = True,    # Enables QLoRA\n",
    "    # processor_class = AutoProcessor, # Unsloth can infer this for common models\n",
    "    # model_class = AutoModelForImageTextToText, # Unsloth can infer this for common models\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4116f92-e548-4673-bcd0-a00daabe7f00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:53:21.178176Z",
     "iopub.status.busy": "2025-07-12T02:53:21.177968Z",
     "iopub.status.idle": "2025-07-12T02:53:29.991307Z",
     "shell.execute_reply": "2025-07-12T02:53:29.990473Z",
     "shell.execute_reply.started": "2025-07-12T02:53:21.178159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "# Unsloth handles modules_to_save internally for `lm_head` and `embed_tokens` for multimodal models.\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    # finetune_vision_layers     = True,  # Turn off for just text!\n",
    "    # finetune_language_layers   = True,  # Should leave on!\n",
    "    # finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    # finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "    target_modules = \"all-linear\",\n",
    "    r = 16,\n",
    "    lora_alpha = 32,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = 2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84bb833-b5ea-4913-9450-10486466b305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:53:29.992550Z",
     "iopub.status.busy": "2025-07-12T02:53:29.992278Z",
     "iopub.status.idle": "2025-07-12T02:53:30.007641Z",
     "shell.execute_reply": "2025-07-12T02:53:30.007114Z",
     "shell.execute_reply.started": "2025-07-12T02:53:29.992531Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21,135,360 || all params: 5,460,573,632 || trainable%: 0.3871\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_AuqUq4dyanS",
   "metadata": {
    "id": "_AuqUq4dyanS"
   },
   "source": [
    "<a id=\"6-training\"></a>\n",
    "\n",
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333aa84e-5209-4d0f-92cf-7dbc46f04634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T08:12:56.367674Z",
     "iopub.status.busy": "2025-07-12T08:12:56.367395Z",
     "iopub.status.idle": "2025-07-12T08:12:56.373449Z",
     "shell.execute_reply": "2025-07-12T08:12:56.372387Z",
     "shell.execute_reply.started": "2025-07-12T08:12:56.367646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# import os\n",
    "\n",
    "# base = snapshot_download(\n",
    "#     repo_id=hub_model_id,               # e.g. \"sdvsvs/gemma3n-e2b-cddm-finetune\"\n",
    "#     allow_patterns=[\"last-checkpoint/*\"], \n",
    "#     token=hf_token\n",
    "# )\n",
    "# src = os.path.join(base, \"last-checkpoint\")\n",
    "# dst = \"/kaggle/working/gemma-qlora-finetuned-cddm/checkpoint-5500\"\n",
    "\n",
    "# import shutil\n",
    "# if os.path.isdir(dst):\n",
    "#     shutil.rmtree(dst)\n",
    "# shutil.copytree(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74570117-648b-4217-ad98-d942b56b07f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:53:30.008692Z",
     "iopub.status.busy": "2025-07-12T02:53:30.008476Z",
     "iopub.status.idle": "2025-07-12T02:53:30.025500Z",
     "shell.execute_reply": "2025-07-12T02:53:30.024819Z",
     "shell.execute_reply.started": "2025-07-12T02:53:30.008678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # 1. Extract all texts and all images from the batch of examples.\n",
    "    all_texts = []\n",
    "    all_images = []\n",
    "    for ex in examples:\n",
    "        # Get the list of PIL images for the current example.\n",
    "        images = process_vision_info(ex[\"messages\"])\n",
    "        all_images.append(images)\n",
    "\n",
    "        # Apply the chat template to convert the messages list into a single string.\n",
    "        text = processor.apply_chat_template(\n",
    "            ex[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        ).strip()\n",
    "        all_texts.append(text)\n",
    "\n",
    "    # 2. Call the processor ONCE on the entire batch.\n",
    "    # This correctly handles tokenization, padding, and image processing for all items at once.\n",
    "    # This is the key fix that resolves the TypeError.\n",
    "    batch = processor(\n",
    "        text=all_texts,\n",
    "        images=all_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # 3. Create the labels tensor for calculating loss.\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Get the token IDs for the start of a model's turn and the end of a turn.\n",
    "    # This helps us identify exactly which parts of the sequence to mask.\n",
    "    model_prompt_start_tokens = processor.tokenizer.encode(\"<start_of_turn>model\\n\", add_special_tokens=False)\n",
    "    end_of_turn_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "    # Iterate through each sequence in the batch to apply the loss mask.\n",
    "    for i in range(len(labels)):\n",
    "        # By default, we want to ignore all tokens when calculating the loss.\n",
    "        ignore_mask = torch.ones_like(labels[i], dtype=torch.bool)\n",
    "        \n",
    "        current_sequence_list = labels[i].tolist()\n",
    "\n",
    "        # Find all occurrences of the model's turn start sequence.\n",
    "        for j in range(len(current_sequence_list)):\n",
    "            if current_sequence_list[j : j + len(model_prompt_start_tokens)] == model_prompt_start_tokens:\n",
    "                # We've found the start of an assistant's response.\n",
    "                response_start_index = j + len(model_prompt_start_tokens)\n",
    "                \n",
    "                # Now, find the end of this response.\n",
    "                try:\n",
    "                    # Search for the next end-of-turn token *after* the response starts.\n",
    "                    response_end_index = current_sequence_list.index(end_of_turn_id, response_start_index)\n",
    "                except ValueError:\n",
    "                    # If no EOS token is found (e.g., it's the end of the sequence), go to the end.\n",
    "                    response_end_index = len(current_sequence_list) - 1\n",
    "                \n",
    "                # Unmask the region corresponding to the assistant's response.\n",
    "                # We calculate loss from the start of the response content to the end token (inclusive).\n",
    "                ignore_mask[response_start_index : response_end_index + 1] = False\n",
    "\n",
    "        # Apply the final mask. All tokens marked True in ignore_mask will be set to -100.\n",
    "        labels[i][ignore_mask] = -100\n",
    "\n",
    "    # Add the correctly masked labels to our batch dictionary.\n",
    "    batch[\"labels\"] = labels\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tH6SQ59SXp9L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T02:53:30.026423Z",
     "iopub.status.busy": "2025-07-12T02:53:30.026227Z",
     "iopub.status.idle": "2025-07-12T02:53:30.416355Z",
     "shell.execute_reply": "2025-07-12T02:53:30.415588Z",
     "shell.execute_reply.started": "2025-07-12T02:53:30.026400Z"
    },
    "id": "tH6SQ59SXp9L",
    "outputId": "22b2f301-ec21-4ff2-e142-68ea2f1af5b8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up the training arguments using transformers.TrainingArguments\n",
    "# This replaces trl.SFTConfig, as Unsloth enhances the standard Trainer.\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir, # directory to save and repository id\n",
    "    num_train_epochs=1, # number of training epochs (Previous: 3)\n",
    "    per_device_train_batch_size=1, # batch size per device during training (Previous: 1)\n",
    "    gradient_accumulation_steps=8, # number of steps before performing a backward/update pass (Previous: 4)\n",
    "    max_steps=8750,\n",
    "    gradient_checkpointing=True, # use gradient checkpointing to save memory (Previous: True)\n",
    "    optim=\"adamw_8bit\", # Use 8-bit AdamW for memory efficiency with Unsloth (Previous: adamw_torch_fused)\n",
    "    logging_steps=25, # log every 10 steps (Previous: 10)\n",
    "    save_strategy=\"steps\", # NEW: Save checkpoints every `save_steps` (Previous: \"epoch\")\n",
    "    save_steps=250, # NEW: Number of steps between saves (adjust based on dataset size/training time)\n",
    "    save_total_limit=2, # NEW: Limit the total number of checkpoints to save\n",
    "    save_only_model=False,\n",
    "    learning_rate=2e-4, # learning rate, based on QLoRA paper (Previous: 2e-4)\n",
    "    fp16=True, # Use fp16 for training if bf16 is not supported or desired\n",
    "    bf16=False, # Explicitly set bf16 to False\n",
    "    max_grad_norm=0.3, # max gradient norm based on QLoRA paper (Previous: 0.3)\n",
    "    warmup_ratio=0.03, # warmup ratio based on QLoRA paper (Previous: 0.03)\n",
    "    lr_scheduler_type=\"constant\", # use constant learning rate scheduler (Previous: \"constant\")\n",
    "    push_to_hub=True, # Change to True if you want to push to HF Hub (Previous: False)\n",
    "    hub_model_id=hub_model_id, # NEW: Your Hugging Face Hub repository name\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_token=hf_token,\n",
    "    hub_private_repo=hub_private_repo, # NEW: Set to True for a private repo\n",
    "    report_to=\"tensorboard\", # report metrics to tensorboard (Previous: \"tensorboard\")\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # use reentrant checkpointing (Previous: same)\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer (from transformers)\n",
    "from transformers import Trainer # Explicitly import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset, \n",
    "    data_collator=collate_fn,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9d1b14-c985-4b56-92d8-c7d594a55b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:15:40.605864Z",
     "iopub.status.busy": "2025-07-11T10:15:40.605613Z",
     "iopub.status.idle": "2025-07-11T10:15:40.611021Z",
     "shell.execute_reply": "2025-07-11T10:15:40.610399Z",
     "shell.execute_reply.started": "2025-07-11T10:15:40.605840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "10.988 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eQDjGWm_G4gs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T02:53:30.417341Z",
     "iopub.status.busy": "2025-07-12T02:53:30.417141Z",
     "iopub.status.idle": "2025-07-12T08:10:54.594046Z",
     "shell.execute_reply": "2025-07-12T08:10:54.592884Z",
     "shell.execute_reply.started": "2025-07-12T02:53:30.417326Z"
    },
    "id": "eQDjGWm_G4gs",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting QLoRA fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tper_device_train_batch_size: 1 (from args) != 0 (from trainer_state.json)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8750' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8750/8750 5:16:09, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7525</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7575</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7625</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7675</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7725</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7775</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7825</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7875</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.100900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7925</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7975</td>\n",
       "      <td>0.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8025</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8075</td>\n",
       "      <td>0.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8125</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.068400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8175</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8225</td>\n",
       "      <td>0.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8275</td>\n",
       "      <td>0.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8325</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8375</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8425</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8475</td>\n",
       "      <td>0.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8525</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8575</td>\n",
       "      <td>0.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8625</td>\n",
       "      <td>0.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8675</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8725</td>\n",
       "      <td>0.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start the fine-tuning process\n",
    "print(\"\\nStarting QLoRA fine-tuning...\")\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lk9o3FHUyanV",
   "metadata": {
    "id": "Lk9o3FHUyanV"
   },
   "source": [
    "\n",
    "<a id=\"7-saving-model\"></a>\n",
    "# 7. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586c427-ae7a-4e2d-bff2-c724de56ac15",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_folder = \"/kaggle/working/full_checkpoint_22k_sample\"\n",
    "os.makedirs(final_folder, exist_ok=True)\n",
    "\n",
    "# 4a) Save model/adapters/tokenizer/trainer state\n",
    "trainer.save_model(final_folder)\n",
    "# tokenizer.save_pretrained(final_folder)\n",
    "trainer.save_state()\n",
    "\n",
    "# 4b) Optionally save optimizer & scheduler state dicts\n",
    "torch.save(trainer.optimizer.state_dict(), os.path.join(final_folder, \"optimizer_state.pt\"))\n",
    "torch.save(trainer.lr_scheduler.state_dict(), os.path.join(final_folder, \"scheduler_state.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219219a-b6e9-4789-8400-9fb35222bcac",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"/kaggle/working/full_checkpoint_22k_sample\",\n",
    "    repo_id=hub_model_id,\n",
    "    repo_type=\"model\",      \n",
    "    path_in_repo=\"full_checkpoint_22k_sample\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b09532-7770-4bd4-83fb-e1ce5ad3ec1f",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"8-inference\"></a>\n",
    "# 8. Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e46fb-b3ff-4568-9cc0-92aca83203c8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "# 1Ô∏è‚É£ Enable fast vision inference\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# 2Ô∏è‚É£ Prepare your single example\n",
    "# Load the image from disk (or reuse one from your dataset)\n",
    "from PIL import Image\n",
    "img=process_vision_info(train_dataset[0][\"messages\"])\n",
    "# Build the messages chat structure:\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant specialized in crop disease diagnosis.\"}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": img},\n",
    "        {\"type\": \"text\",  \"text\": \"what diesis does this leaf has?\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Process inputs\n",
    "inputs = processor(\n",
    "    text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "    images=[ img ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "# 4Ô∏è‚É£ Generate output\n",
    "streamer = TextStreamer(processor.tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=1.0,\n",
    "    top_k=64,\n",
    "    top_p=0.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5483f51-f156-4458-b98d-8ec6ae6533a6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958017a6-8b5c-4f9a-b60c-822995718ab7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img=process_vision_info(train_dataset[0][\"messages\"])\n",
    "img=np.squeeze(img)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7788520,
     "sourceId": 12353874,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
