{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b431db4d",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8684fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth==2025.7.3 \n",
    "!pip install unsloth_zoo==2025.7.3\n",
    "!pip install --upgrade transformers timm peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452c033",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63374e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import re\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=3407):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e465d",
   "metadata": {},
   "source": [
    "# 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/gemma-3n-e2b-it-unsloth-bnb-4bit\" # Using Unsloth's pre-quantized version for efficiency\n",
    "output_dir = \"./gemma-qlora-finetuned-cddm\"\n",
    "\n",
    "hub_model_id = \"shreyansh24/gemma3n-e2b-cddm-finetune\"\n",
    "hub_private_repo = False # Set to True if you want your model repository to be private on the Hub.\n",
    "\n",
    "# Hugging Face Hub repository ID for the UNIVERSAL DATASET\n",
    "hub_dataset_name = \"shreyansh24/Crop-Disease-VQA\" # Your new dataset name\n",
    "\n",
    "# Secure token handling\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or getpass(\"Enter Hugging Face token: \")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN environment variable or token input required\")\n",
    "\n",
    "login(token=hf_token) # Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cab6e7",
   "metadata": {},
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265cbbe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Make paths work both on Kaggle and locally\n",
    "KAGGLE_INPUT_DIR = \"/kaggle/input/crop-disease-data/\"\n",
    "if not os.path.exists(KAGGLE_INPUT_DIR):\n",
    "    KAGGLE_INPUT_DIR = \"./crop-disease-data/\"\n",
    "    logger.info(f\"Using local data directory: {KAGGLE_INPUT_DIR}\")\n",
    "\n",
    "dataset_json_path = os.path.join(KAGGLE_INPUT_DIR, \"dataset/Crop_Disease_train_qwenvl.json\")\n",
    "system_message = \"You are a helpful AI assistant specialized in crop disease diagnosis. Provide concise and accurate information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab33e0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Helper function to extract PIL Images from the TRL-formatted messages\n",
    "# This is used by the `collate_fn`.\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    \"\"\"Extract PIL Images from TRL-formatted messages with error handling\"\"\"\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and element.get(\"type\") == \"image\":\n",
    "                if \"image\" in element and isinstance(element[\"image\"], Image.Image):\n",
    "                    try:\n",
    "                        image_inputs.append(element[\"image\"].convert(\"RGB\"))\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to convert image to RGB: {e}\")\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e9ce5",
   "metadata": {},
   "source": [
    "The raw CDDM dataset uses a specific format with `<img>` tags inside the text. The `format_data_for_trl` function is a custom parser that transforms this into the standard TRL multimodal chat format. It extracts the image path, loads the PIL Image object, and constructs the conversation turns that Unsloth and Gemma can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9bca7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to transform CDDM format to TRL multimodal chat format (Gemma style)\n",
    "def format_data_for_trl(sample):\n",
    "    \"\"\"Transform CDDM format to TRL multimodal chat format with error handling\"\"\"\n",
    "    trl_messages = []\n",
    "\n",
    "    try:\n",
    "        # 1. Add system message at the beginning of each conversation\n",
    "        trl_messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "        })\n",
    "\n",
    "        # Process each turn in the conversation\n",
    "        for i, turn in enumerate(sample['conversations']):\n",
    "            # Map Qwen-VL roles to Gemma roles\n",
    "            current_role = \"user\" if turn[\"from\"] == \"user\" else \"assistant\"\n",
    "            content_value = turn[\"value\"]\n",
    "\n",
    "            content_parts = []\n",
    "\n",
    "            img_pattern = r'<img>(.*?)</img>'\n",
    "            img_match = re.search(img_pattern, content_value)\n",
    "\n",
    "            # Handle the case where an image is present in the user's turn\n",
    "            if img_match and current_role == \"user\":\n",
    "                image_path_in_text = img_match.group(1)\n",
    "\n",
    "                # Construct the ABSOLUTE image path from Kaggle Input\n",
    "                cleaned_image_path_in_text = image_path_in_text.lstrip('/')\n",
    "                corrected_image_path = os.path.join(KAGGLE_INPUT_DIR, cleaned_image_path_in_text)\n",
    "\n",
    "                loaded_image = None\n",
    "                # Use the correctly constructed path to check if the file exists\n",
    "                if os.path.exists(corrected_image_path):\n",
    "                    try:\n",
    "                        # Try opening the image file\n",
    "                        loaded_image = Image.open(corrected_image_path).convert(\"RGB\")\n",
    "                        content_parts.append({\"type\": \"image\", \"image\": loaded_image})\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Could not load image {corrected_image_path}: {e}\")\n",
    "                        content_parts.append({\"type\": \"text\", \"text\": f\"Error loading image: {corrected_image_path}\"})\n",
    "                else:\n",
    "                    logger.warning(f\"Image file not found at {corrected_image_path}\")\n",
    "                    content_parts.append({\"type\": \"text\", \"text\": f\"Image not found: {corrected_image_path}\"})\n",
    "\n",
    "                # Extract text after removing the <img> tag and \"Picture X:\" prefix\n",
    "                text_part = re.sub(img_pattern, \"\", content_value)\n",
    "                text_part = re.sub(r'^(Picture\\s+\\d+:\\s*)', '', text_part).strip()\n",
    "\n",
    "                if text_part:\n",
    "                    content_parts.append({\"type\": \"text\", \"text\": text_part})\n",
    "                elif not content_parts and loaded_image is None:\n",
    "                     content_parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "            else:\n",
    "                # All other turns are text-only\n",
    "                cleaned_text = turn[\"value\"].strip()\n",
    "                if cleaned_text:\n",
    "                    content_parts.append({\"type\": \"text\", \"text\": cleaned_text})\n",
    "                elif current_role == \"assistant\":\n",
    "                    content_parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "            if content_parts:\n",
    "                trl_messages.append({\"role\": current_role, \"content\": content_parts})\n",
    "\n",
    "        # Ensure the messages list is not empty\n",
    "        if not trl_messages:\n",
    "            return {\"messages\": [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}]}\n",
    "\n",
    "        return {\"messages\": trl_messages}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing sample {sample.get('id', 'N/A')}: {e}\")\n",
    "        return {\"messages\": [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the JSON file with error handling\n",
    "try:\n",
    "    with open(dataset_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    logger.info(f\"Successfully loaded {len(data)} samples from dataset\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Dataset file not found: {dataset_json_path}\")\n",
    "    raise\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"Invalid JSON format in: {dataset_json_path}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use subset for testing (comment out for full training)\n",
    "# start_subset=110000 \n",
    "# end_subset = 120000\n",
    "# data=data[start_subset:end_subset]\n",
    "\n",
    "# Format the dataset\n",
    "logger.info(\"Formatting dataset for training...\")\n",
    "train_dataset = [format_data_for_trl(sample) for sample in data]\n",
    "logger.info(f\"Formatted {len(train_dataset)} training samples\")\n",
    "\n",
    "# Show first sample\n",
    "if train_dataset:\n",
    "    logger.info(\"Sample training data structure:\")\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a sample with an image in a user turn and print its content\n",
    "print(f\"\\nExample of image loading for a user turn:\")\n",
    "image_sample_found = False\n",
    "for sample in train_dataset:\n",
    "    for msg in sample['messages']:\n",
    "        if msg['role'] == 'user':\n",
    "            for content_elem in msg['content']:\n",
    "                if content_elem['type'] == 'image':\n",
    "                    print(f\"  Image element found. PIL Image object: {content_elem['image']}\")\n",
    "                    image_sample_found = True\n",
    "                    break\n",
    "        if image_sample_found:\n",
    "            break\n",
    "    if image_sample_found:\n",
    "        break\n",
    "if not image_sample_found:\n",
    "    print(\"No image found in a user turn in the first few samples to demonstrate PIL object loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a2c43",
   "metadata": {},
   "source": [
    "# 5. Model Loading and QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading base model...\")\n",
    "try:\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name = model_id,\n",
    "        max_seq_length = 2048,\n",
    "        dtype = torch.float16,\n",
    "        load_in_4bit = True, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    logger.info(\"Base model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load base model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get tokenizer for later use\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Applying QLoRA configuration...\")\n",
    "try:\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        target_modules = \"all-linear\",\n",
    "        r = 16,\n",
    "        lora_alpha = 32,\n",
    "        bias = \"none\",\n",
    "        use_gradient_checkpointing = True,\n",
    "        random_state = 3407,\n",
    "        max_seq_length = 2048,\n",
    "    )\n",
    "    logger.info(\"QLoRA configuration applied successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to apply QLoRA: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6687e6f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "logger.info(\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f259015",
   "metadata": {},
   "source": [
    "# 6. Training\n",
    "\n",
    "The `collate_fn` is the heart of the data preparation for training. It takes a batch of formatted samples and does two critical things.</br>\n",
    "Processes Data: It uses the `processor` to tokenize the text and handle the images for the entire batch at once.</br>\n",
    "Creates Labels: It intelligently masks the input text (user questions and system prompts) so that the model only learns to predict the assistant's answers. This is the correct way to fine-tune a conversational model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288fa48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for training with error handling\"\"\"\n",
    "    try:\n",
    "        # 1. Extract all texts and all images from the batch of examples.\n",
    "        all_texts = []\n",
    "        all_images = []\n",
    "        for ex in examples:\n",
    "            # Get the list of PIL images for the current example.\n",
    "            images = process_vision_info(ex[\"messages\"])\n",
    "            all_images.append(images)\n",
    "\n",
    "            # Apply the chat template to convert the messages list into a single string.\n",
    "            text = processor.apply_chat_template(\n",
    "                ex[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "            ).strip()\n",
    "            all_texts.append(text)\n",
    "\n",
    "        # 2. Call the processor ONCE on the entire batch.\n",
    "        batch = processor(\n",
    "            text=all_texts,\n",
    "            images=all_images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # 3. Create the labels tensor for calculating loss.\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # Get the token IDs for the start of a model's turn and the end of a turn.\n",
    "        model_prompt_start_tokens = processor.tokenizer.encode(\"<start_of_turn>model\\n\", add_special_tokens=False)\n",
    "        end_of_turn_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "        # Iterate through each sequence in the batch to apply the loss mask.\n",
    "        for i in range(len(labels)):\n",
    "            # By default, we want to ignore all tokens when calculating the loss.\n",
    "            ignore_mask = torch.ones_like(labels[i], dtype=torch.bool)\n",
    "            \n",
    "            current_sequence_list = labels[i].tolist()\n",
    "\n",
    "            # Find all occurrences of the model's turn start sequence.\n",
    "            for j in range(len(current_sequence_list)):\n",
    "                if current_sequence_list[j : j + len(model_prompt_start_tokens)] == model_prompt_start_tokens:\n",
    "                    # We've found the start of an assistant's response.\n",
    "                    response_start_index = j + len(model_prompt_start_tokens)\n",
    "                    \n",
    "                    # Now, find the end of this response.\n",
    "                    try:\n",
    "                        # Search for the next end-of-turn token *after* the response starts.\n",
    "                        response_end_index = current_sequence_list.index(end_of_turn_id, response_start_index)\n",
    "                    except ValueError:\n",
    "                        # If no EOS token is found (e.g., it's the end of the sequence), go to the end.\n",
    "                        response_end_index = len(current_sequence_list) - 1\n",
    "                    \n",
    "                    # Unmask the region corresponding to the assistant's response.\n",
    "                    ignore_mask[response_start_index : response_end_index + 1] = False\n",
    "\n",
    "            # Apply the final mask. All tokens marked True in ignore_mask will be set to -100.\n",
    "            labels[i][ignore_mask] = -100\n",
    "\n",
    "        # Add the correctly masked labels to our batch dictionary.\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in collate_fn: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "logger.info(\"Setting up training arguments...\")\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir, # directory to save and repository id\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size per device during training\n",
    "    gradient_accumulation_steps=8, # number of steps before performing a backward/update pass\n",
    "    max_steps=14250,\n",
    "    gradient_checkpointing=True, # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_8bit\", # Use 8-bit AdamW for memory efficiency with Unsloth\n",
    "    logging_steps=25, # log every 25 steps\n",
    "    save_strategy=\"steps\", # Save checkpoints every `save_steps`\n",
    "    save_steps=250, # Number of steps between saves\n",
    "    save_total_limit=2, # Limit the total number of checkpoints to save\n",
    "    save_only_model=False,\n",
    "    learning_rate=2e-4, # learning rate, based on QLoRA paper\n",
    "    fp16=True, # Use fp16 for training if bf16 is not supported or desired\n",
    "    bf16=False, # Explicitly set bf16 to False\n",
    "    max_grad_norm=0.3, # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03, # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\", # use constant learning rate scheduler\n",
    "    push_to_hub=True, # Push to HF Hub\n",
    "    hub_model_id=hub_model_id, # Your Hugging Face Hub repository name\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_token=hf_token,\n",
    "    hub_private_repo=hub_private_repo, # Set to True for a private repo\n",
    "    report_to=\"tensorboard\", # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # use reentrant checkpointing\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "logger.info(\"Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset, \n",
    "    data_collator=collate_fn,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the fine-tuning process\n",
    "latest_checkpoint = None\n",
    "if os.path.isdir(args.output_dir):\n",
    "    checkpoints = [d for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = os.path.join(\n",
    "            args.output_dir, max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "        )\n",
    "        logger.info(f\"Resuming training from latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "logger.info(\"Starting QLoRA fine-tuning...\")\n",
    "try:\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "    logger.info(\"Training completed successfully\")\n",
    "    \n",
    "    # Print training metrics\n",
    "    if hasattr(trainer_stats, 'training_loss'):\n",
    "        logger.info(f\"Final training loss: {trainer_stats.training_loss[-1]:.4f}\")\n",
    "    if hasattr(trainer_stats, 'total_flos'):\n",
    "        logger.info(f\"Total training time: {trainer_stats.total_flos/1e12:.2f} TFLOPS\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea139bc",
   "metadata": {},
   "source": [
    "# 7. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving final model...\")\n",
    "final_folder = \"/kaggle/working/full_checkpoint\"\n",
    "os.makedirs(final_folder, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save model/adapters/tokenizer/trainer state\n",
    "    trainer.save_model(final_folder)\n",
    "    tokenizer.save_pretrained(final_folder)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # Save optimizer & scheduler state dicts\n",
    "    torch.save(trainer.optimizer.state_dict(), os.path.join(final_folder, \"optimizer_state.pt\"))\n",
    "    torch.save(trainer.lr_scheduler.state_dict(), os.path.join(final_folder, \"scheduler_state.pt\"))\n",
    "    \n",
    "    logger.info(\"Model saved successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5733b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face Hub\n",
    "logger.info(f\"Uploading model to Hugging Face Hub: {hub_model_id}\")\n",
    "try:\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=\"/kaggle/working/full_checkpoint\",\n",
    "        repo_id=hub_model_id,\n",
    "        repo_type=\"model\",\n",
    "        path_in_repo=\"full_checkpoint\"\n",
    "    )\n",
    "    logger.info(\"Model uploaded successfully to Hugging Face Hub\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to upload model to Hub: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508fdcb",
   "metadata": {},
   "source": [
    "# 8. Inference\n",
    "After fine-tuning, let's test our model's new capabilities. This section loads the fully trained model and runs a sample inference on an image from our dataset to verify its diagnostic abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Setting up inference...\")\n",
    "try:\n",
    "    from unsloth import FastVisionModel\n",
    "    import torch\n",
    "    from transformers import TextStreamer\n",
    "\n",
    "    # Enable fast vision inference\n",
    "    FastVisionModel.for_inference(model)\n",
    "\n",
    "    # Prepare your single example\n",
    "    img = process_vision_info(train_dataset[0][\"messages\"])\n",
    "    \n",
    "    # Build the messages chat structure:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant specialized in crop disease diagnosis.\"}]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\",  \"text\": \"what disease does this leaf has?\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # Process inputs\n",
    "    inputs = processor(\n",
    "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "        images=[img],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    logger.info(\"Generating inference...\")\n",
    "    streamer = TextStreamer(processor.tokenizer)\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=128,\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Inference completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Inference failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample image for verification\n",
    "logger.info(\"Displaying sample image...\")\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    img = process_vision_info(train_dataset[0][\"messages\"])\n",
    "    if img:\n",
    "        img_array = np.squeeze(img)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img_array)\n",
    "        plt.title(\"Sample Training Image\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        logger.info(\"Sample image displayed successfully\")\n",
    "    else:\n",
    "        logger.warning(\"No image found to display\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to display image: {e}\")\n",
    "\n",
    "logger.info(\"🎉 Fine-tuning pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
