{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d077a0b0",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb319cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --upgrade transformers timm peft\n",
    "!pip install jinja2==3.1.0 rarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bcd740",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from unsloth import FastVisionModel\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import login, hf_hub_download, HfApi\n",
    "import rarfile\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=3407):    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf2fb2",
   "metadata": {},
   "source": [
    "# 3. Configuration\n",
    "This section sets up all the key configurations for our training run, including model IDs, output directories, and Hugging Face authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19588e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "base_model_id = \"unsloth/gemma-3n-e2b-it-unsloth-bnb-4bit\"\n",
    "previous_adapter_hub_id = \"shreyansh24/gemma3n-e2b-cddm-finetune\"\n",
    "\n",
    "# Training configuration\n",
    "output_dir = \"./gemma-cddm-agrillava-finetuned\"\n",
    "hub_model_id = \"shreyansh24/gemma3n-cddm-AgriLLava-finetune\"\n",
    "hub_repo_id = \"shreyansh24/AgriVision-gemma3n\"\n",
    "hub_private_repo = False\n",
    "\n",
    "# Secure token handling\n",
    "hf_token = os.getenv(\"HF_TOKEN\") or getpass(\"Enter Hugging Face token: \")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN environment variable or token input required\")\n",
    "\n",
    "login(token=hf_token)\n",
    "\n",
    "# System message\n",
    "system_message = \"You are a helpful AI assistant specialized in crop disease diagnosis. Provide concise and accurate information.\"\n",
    "print(\"‚úÖ Configuration and login complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da12203",
   "metadata": {},
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üöú Preparing Agri-LLaVA Dataset ---\")\n",
    "\n",
    "# --- 4.1. Download and Extract Agri-LLaVA Data ---\n",
    "agri_repo = \"Agri-LLaVA-Anonymous/Agricultural_pests_and_diseases_instruction_tuning_data\"\n",
    "data_dir = \"./agri_llava_data/\"  # Changed to relative path\n",
    "image_dir = os.path.join(data_dir, \"Img\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download function with error handling\n",
    "def download_hf_file(filename, repo, local_dir, max_retries=3):\n",
    "    \"\"\"Safe download with retry logic\"\"\"\n",
    "    path = os.path.join(local_dir, filename)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Downloading {filename}...\")\n",
    "                hf_hub_download(repo_id=repo, filename=filename, repo_type=\"dataset\", local_dir=local_dir)\n",
    "            else:\n",
    "                print(f\"{filename} already exists.\")\n",
    "            return path\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            print(f\"Retry {attempt+1}/{max_retries} for {filename}\")\n",
    "            time.sleep(2)\n",
    "    return path\n",
    "\n",
    "# Download components\n",
    "try:\n",
    "    json_full_path = download_hf_file(\"agri_llava_instruction_tuning.json\", agri_repo, data_dir)\n",
    "    json_1k_path = download_hf_file(\"agri_llava_instruction_tuning_1k.json\", agri_repo, data_dir)\n",
    "    rar_path = download_hf_file(\"Img.rar\", agri_repo, data_dir)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to download files: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract images with error handling\n",
    "if not os.path.exists(image_dir):\n",
    "    try:\n",
    "        print(f\"Extracting {rar_path}...\")\n",
    "        with rarfile.RarFile(rar_path) as rf:\n",
    "            rf.extractall(data_dir)\n",
    "        print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract images: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Image directory already exists.\")\n",
    "\n",
    "# --- 4.2. Load and Merge JSON files ---\n",
    "try:\n",
    "    with open(json_full_path, 'r') as f: \n",
    "        data_full = json.load(f)\n",
    "    with open(json_1k_path, 'r') as f: \n",
    "        data_1k = json.load(f)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load JSON files: {e}\")\n",
    "    raise\n",
    "\n",
    "# Combine and remove duplicates, then convert to Hugging Face Dataset\n",
    "df = pd.concat([pd.DataFrame(data_full), pd.DataFrame(data_1k)], ignore_index=True)\n",
    "df.drop_duplicates(subset=['image'], inplace=True, keep='first')\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Loaded and merged JSONs. Total unique samples: {len(raw_dataset)}\")\n",
    "\n",
    "def sample_has_image(sample):\n",
    "    \"\"\"Checks if the image file for a sample exists on disk.\"\"\"\n",
    "    if not sample['image']:\n",
    "        return False\n",
    "    image_path = os.path.join(image_dir, sample['image'])\n",
    "    return os.path.exists(image_path)\n",
    "\n",
    "print(\"\\nFiltering dataset to include only samples with valid images...\")\n",
    "image_dataset_raw = raw_dataset.filter(sample_has_image, num_proc=2)\n",
    "print(f\"Original dataset size: {len(raw_dataset)}. Filtered image-only dataset size: {len(image_dataset_raw)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d9552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = (512, 512)  # or (256, 256)\n",
    "\n",
    "def format_agri_data_for_trl(sample):\n",
    "    \"\"\"Format data for TRL training with error handling\"\"\"\n",
    "    image_path = os.path.join(image_dir, sample['image'])\n",
    "    loaded_image = None\n",
    "    try:\n",
    "        loaded_image = Image.open(image_path).convert(\"RGB\")\n",
    "        loaded_image = loaded_image.resize(TARGET, resample=Image.BILINEAR)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    trl_messages = [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}]\n",
    "    for i, turn in enumerate(sample['conversations']):\n",
    "        role = \"user\" if turn[\"from\"] == \"human\" else \"assistant\"\n",
    "        text_content = turn[\"value\"]\n",
    "        content_parts = []\n",
    "        if role == \"user\" and i == 0 and loaded_image:\n",
    "            content_parts.append({\"type\": \"image\", \"image\": loaded_image})\n",
    "            text_content = text_content.replace(\"<image>\", \"\").strip()\n",
    "        if text_content:\n",
    "            content_parts.append({\"type\": \"text\", \"text\": text_content})\n",
    "        if not content_parts:\n",
    "            content_parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "        trl_messages.append({\"role\": role, \"content\": content_parts})\n",
    "    return {\"messages\": trl_messages}\n",
    "\n",
    "# Apply formatting and build the final dataset list\n",
    "print(\"\\nApplying formatting and loading images directly into a list...\")\n",
    "train_dataset = [format_agri_data_for_trl(sample) for sample in image_dataset_raw]\n",
    "train_dataset = [item for item in train_dataset if item is not None]\n",
    "print(f\"Final dataset size: {len(train_dataset)}\")\n",
    "print(\"Sample data structure:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298e264",
   "metadata": {},
   "source": [
    "# 5. Model Loading and QLoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d29cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- ‚öôÔ∏è Loading Model and Adapter (Final) ---\")\n",
    "\n",
    "try:\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name = base_model_id,\n",
    "        max_seq_length = 2048,\n",
    "        dtype = torch.float16,\n",
    "        load_in_4bit = True,\n",
    "        trust_remote_code=True,\n",
    "        unsloth_force_compile = True,  \n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load base model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Applying adapter from: {previous_adapter_hub_id}...\")\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, previous_adapter_hub_id, is_trainable=True)\n",
    "    print(\"‚úÖ Previous adapter has been applied successfully.\")\n",
    "    model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load adapter: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d705f",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa58372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    \"\"\"Extract image information from messages\"\"\"\n",
    "    image_inputs = []\n",
    "    for msg in messages:\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and element.get(\"type\") == \"image\":\n",
    "                if \"image\" in element and isinstance(element[\"image\"], Image.Image):\n",
    "                    image_inputs.append(element[\"image\"].convert(\"RGB\"))\n",
    "    return image_inputs\n",
    "\n",
    "def format_messages_to_gemma_chat(messages):\n",
    "    \"\"\"Manually format messages into Gemma's chat format without Jinja2\"\"\"\n",
    "    formatted = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content_parts = []\n",
    "        \n",
    "        # Process ALL content parts (both text AND image markers)\n",
    "        for part in message[\"content\"]:\n",
    "            if part[\"type\"] == \"text\":\n",
    "                content_parts.append(part[\"text\"])\n",
    "            elif part[\"type\"] == \"image\":\n",
    "                # CRITICAL: Add the special image token that the model recognizes\n",
    "                content_parts.append(\"<image>\")\n",
    "        \n",
    "        content = \" \".join(content_parts)\n",
    "        \n",
    "        if role == \"system\":\n",
    "            formatted += f\"<start_of_turn>system\\n{content}<end_of_turn>\\n\"\n",
    "        elif role == \"user\":\n",
    "            formatted += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            formatted += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
    "    return formatted.strip()\n",
    "\n",
    "# The `collate_fn` is a crucial function that prepares batches of data for the trainer. \n",
    "# Its main job is to correctly format the text and images. \n",
    "# Most importantly, it creates the `labels` for training by masking out the user's questions and the system prompt.\n",
    "# This teaches the model to predict only the assistant's responses, which is the standard and correct way to fine-tune a conversational AI.\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Custom collate function for training\"\"\"\n",
    "    all_texts = []\n",
    "    all_images = []\n",
    "    for ex in examples:\n",
    "        images_for_sample = process_vision_info(ex[\"messages\"])\n",
    "        all_images.append(images_for_sample)\n",
    "        text = processor.apply_chat_template(\n",
    "            ex[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        ).strip()\n",
    "        all_texts.append(text)\n",
    "    \n",
    "    batch = processor(\n",
    "        text=all_texts,\n",
    "        images=all_images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    model_prompt_start_tokens = processor.tokenizer.encode(\"model\\n\", add_special_tokens=False)\n",
    "    end_of_turn_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        ignore_mask = torch.ones_like(labels[i], dtype=torch.bool)\n",
    "        current_sequence_list = labels[i].tolist()\n",
    "        for j in range(len(current_sequence_list)):\n",
    "            if current_sequence_list[j : j + len(model_prompt_start_tokens)] == model_prompt_start_tokens:\n",
    "                response_start_index = j + len(model_prompt_start_tokens)\n",
    "                try:\n",
    "                    response_end_index = current_sequence_list.index(end_of_turn_id, response_start_index)\n",
    "                except ValueError:\n",
    "                    response_end_index = len(current_sequence_list) - 1\n",
    "                ignore_mask[response_start_index : response_end_index + 1] = False\n",
    "        labels[i][ignore_mask] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=-1, \n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    save_only_model=False,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    hub_token=hf_token,\n",
    "    hub_private_repo=hub_private_repo,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Configure PyTorch optimizations\n",
    "torch._dynamo.config.cache_size_limit = 512\n",
    "torch._dynamo.config.automatic_dynamic_shapes = True\n",
    "torch._dynamo.config.recompile_limit = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148367c3",
   "metadata": {},
   "source": [
    "# 7. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae656446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üöÄ Starting Continued Fine-tuning ---\")\n",
    "try:\n",
    "    trainer_stats = trainer.train()\n",
    "    print(\"\\n--- ‚úÖ Training Complete ---\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    if hasattr(trainer_stats, 'training_loss'):\n",
    "        print(f\"Final training loss: {trainer_stats.training_loss[-1]:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35abd505",
   "metadata": {},
   "source": [
    "# 8. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üíæ Saving Final Model ---\")\n",
    "\n",
    "# Set up paths\n",
    "adapter_path = output_dir\n",
    "merged_model_path = \"./gemma3n-agrivision-merged\"\n",
    "\n",
    "# Load the base model\n",
    "print(\"Loading base model...\")\n",
    "try:\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=2048,\n",
    "        dtype=torch.float16,\n",
    "        load_in_4bit=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load base model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load the adapter\n",
    "print(\"Loading adapter...\")\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(\"Adapter loaded successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load adapter: {e}\")\n",
    "    raise\n",
    "\n",
    "# Merge the adapter with the base model\n",
    "print(\"Merging adapter with base model...\")\n",
    "try:\n",
    "    merged_model = model.merge_and_unload()\n",
    "    print(\"Model merged successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to merge model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save the merged model\n",
    "print(f\"Saving merged model to {merged_model_path}...\")\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    merged_model.save_pretrained(merged_model_path)\n",
    "    processor.save_pretrained(merged_model_path)\n",
    "    print(\"Merged model saved successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save merged model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create repository and push to Hub\n",
    "print(f\"Creating repository: {hub_repo_id}\")\n",
    "try:\n",
    "    api = HfApi()\n",
    "    api.create_repo(\n",
    "        repo_id=hub_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        private=hub_private_repo,\n",
    "        exist_ok=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create repository: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Pushing merged model to Hub: {hub_repo_id}...\")\n",
    "try:\n",
    "    api.upload_folder(\n",
    "        folder_path=merged_model_path,\n",
    "        repo_id=hub_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload merged Gemma3N-AgriVision model\"\n",
    "    )\n",
    "    print(\"Model successfully pushed to Hub!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to push model to Hub: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e93d05",
   "metadata": {},
   "source": [
    "# 9. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29656cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ü§ñ Model Inference ---\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install --upgrade transformers timm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# Load model and processor\n",
    "print(f\"Loading model from: {hub_repo_id}\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        hub_repo_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(hub_repo_id)\n",
    "    print(\"‚úÖ Model and processor loaded successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model for inference: {e}\")\n",
    "    raise\n",
    "\n",
    "# Enhanced inference function\n",
    "def run_inference(image_path=None, question=\"What's wrong with this crop?\"):\n",
    "    \"\"\"Complete inference pipeline with optional image\"\"\"\n",
    "    try:\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            # Load and process image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": question}\n",
    "                ]}\n",
    "            ]\n",
    "            inputs = processor(\n",
    "                text=processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False),\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        else:\n",
    "            # Text-only inference\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            inputs = processor(\n",
    "                text=processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False),\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)\n",
    "        \n",
    "        response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response.split(\"model\\n\")[-1].strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Inference failed: {e}\")\n",
    "        return f\"Error during inference: {str(e)}\"\n",
    "\n",
    "# Test inference\n",
    "print(\"\\nüß™ Testing inference...\")\n",
    "test_response = run_inference(question=\"Hii, can you help me identify this crop disease?\")\n",
    "print(f\"Model's Reply: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce2294",
   "metadata": {},
   "source": [
    "# 10. Training Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eaa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(trainer_stats):\n",
    "    \"\"\"Plot training loss and learning rate\"\"\"\n",
    "    if hasattr(trainer_stats, 'training_loss'):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(trainer_stats.training_loss)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        if hasattr(trainer_stats, 'learning_rate'):\n",
    "            plt.plot(trainer_stats.learning_rate)\n",
    "            plt.title('Learning Rate')\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('LR')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.show()\n",
    "        print(\"Training metrics saved as 'training_metrics.png'\")\n",
    "\n",
    "# Uncomment to generate training plots\n",
    "# plot_training_metrics(trainer_stats)\n",
    "\n",
    "print(\"\\nüéâ Fine-tuning pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
